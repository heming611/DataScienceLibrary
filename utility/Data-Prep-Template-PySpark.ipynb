{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2dc9ecac8d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from time import time\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import utility\n",
    "%load_ext blackcellmagic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise sparkContext\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"IQreqTypeSuggester\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Xss10m\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.cores.max\", \"6\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"checkpoint/\")\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.parquet.binaryAsString\", \"true\")\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "start_date = \"2018-11-01\"\n",
    "end_date = \"2019-02-01\"\n",
    "name = \"\"\n",
    "data_path = data_dir + name + \".\" + start_date + \".\" + end_date + \".parquet\"\n",
    "df_name_1 = spark.read.parquet(data_path).persist()\n",
    "\n",
    "\n",
    "#gain of this repartition, or in other words, what can go bad w/o repartition\n",
    "#df.repartition(400).write.mode(\"overwrite\").parquet(data_dir + start_date + '_' + end_date + '-repartitioned.parquet')\n",
    "\n",
    "df_name_1 = df_name_1.drop_duplicates(subset=[\"A\",\"B\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean and explore data\n",
    "**by row**: \n",
    "* check and drop na\n",
    "* drop duplicates\n",
    "\n",
    "**by column**: \n",
    "* number of unique values for categorical column (pay attention to empty string and None for string valued column) \n",
    "* summary statistics for numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_1.printSchema()\n",
    "df_name_1.info()\n",
    "cols_to_describe = [\"A\",\"B\",\"C\"]\n",
    "df_name_1.select(*cols_to_describe).describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potential groupby\n",
    "group_key = ['A','B']\n",
    "aggregation = {'C':'mean', \n",
    "               'D':'sum', \n",
    "               'E':'count'}\n",
    "\n",
    "df_name_1 = df_name_1.groupby(group_key).agg(aggregation).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_1.createOrReplaceTempView(\"df_name_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "start_date = \"2018-11-01\"\n",
    "end_date = \"2019-02-01\"\n",
    "name = \"\"\n",
    "data_path = data_dir + name + \".\" + start_date + \".\" + end_date + \".parquet\"\n",
    "df_name_2 = spark.read.parquet(data_path).persist()\n",
    "\n",
    "#gain of this repartition, or in other words, what can go bad w/o repartition\n",
    "#df.repartition(400).write.mode(\"overwrite\").parquet(data_dir + start_date + '_' + end_date + '-repartitioned.parquet')\n",
    "\n",
    "df_name_2 = df_name_2.drop_duplicates(subset=[\"A\",\"B\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_2.printSchema()\n",
    "cols_to_describe = [\"A\",\"B\",\"C\"]\n",
    "df_name_2.select(*cols_to_describe).describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potential groupby\n",
    "group_key = ['A','B']\n",
    "aggregation = {'C':'mean', \n",
    "               'D':'sum', \n",
    "               'E':'count'}\n",
    "\n",
    "df_name_2 = df_name_2.groupby(group_key).agg(aggregation).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_2.createOrReplaceTempView(\"df_name_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"\"\"\n",
    "  SELECT a.*, b.z as z\n",
    "  FROM df_1 a left join df_2 b on (a.x = b.x) and (a.y = b.y) \n",
    "  \"\"\").persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_fill_scheme = {\"A\": 0, \"B\": 1}\n",
    "df = df.na.fill(na_fill_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#potential groupby\n",
    "group_key = ['A','B']\n",
    "aggregation = {'C':'mean', \n",
    "               'D':'sum', \n",
    "               'E':'count'}\n",
    "\n",
    "df = df.groupby(group_key).agg(aggregation).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.info()\n",
    "cols_to_describe = [\"A\",\"B\",\"C\"]\n",
    "df.select(*cols_to_describe).describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select = [\"A\",\"B\"]\n",
    "df = df.select(*cols_to_select).drop_duplicates().persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: standard operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## column operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column based on existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('A', (1-F.col('B'))*(1-F.col('C')))\n",
    "df = df.withColumn('D', F.lit(1))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"old_name\", \"new_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"new_col\", (df.A>0).cast(\"integer\"))\n",
    "df = df.withColumn(\"new_col\", ((df.A-df.B)>0).cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort according to one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(F.desc(\"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter to create subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.filter(df.A.isin([\"value1\", \"value2\"]) | df.B.isin(['value1','value2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.filter(~df.A.isin([\"value1\", \"value2\"]) & ~df.B.isin(['value1','value2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.filter(F.col('A')>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.where(df.A==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count distinct values of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(*(F.countDistinct(F.col(c)).alias(c) for c in df.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(label, posPercentage):\n",
    "    weight = 1/(posPercentage * 2) if label==1 else 1/((1-posPercentage)*2)\n",
    "    \n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use udf to create new column out of existing columns\n",
    "from pyspark.sql.types import *\n",
    "udf_name = F.udf(fun, returnType = FloatType())\n",
    "df = df.withColumn(\"A\", udf_name('B', F.lit(0.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosetta_category(attributes, category):\n",
    "    '''\n",
    "    functionality: given attributes, to check if it has any attribute in the category\n",
    "    attributes: one long string containing multiple rosetta attributes\n",
    "    category: the category of rosetta attribute\n",
    "    '''\n",
    "    try:\n",
    "        for attribute in attributes:\n",
    "            try: \n",
    "                if attribute[:len(category)] == category:\n",
    "                    return 1\n",
    "            except AttributeError:\n",
    "                continue\n",
    "        return 0\n",
    "    except TypeError:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"verifiedExtractedAttributeList\", F.split(\"verifiedExtractedAttribute\", \",\"))\n",
    "\n",
    "#convert a function into udf\n",
    "udf_rosetta_category = F.udf(rosetta_category, returnType = IntegerType())\n",
    "df = df.withColumn(\"skill\", udf_rosetta_category(\"verifiedExtractedAttributeList\", F.lit(\"skill\")))\n",
    "\n",
    "#UDF is user-defined function to transform each row of a column, and it is like lambda function in python\n",
    "rosetta_categories = ['skill', 'education', 'language', 'license'] \n",
    "for category in rosetta_categories:\n",
    "    df = df.withColumn(category, udf_rosetta_category(\"verifiedExtractedAttributeList\", F.lit(category)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "maturity_udf = udf(lambda age: \"adult\" if age >=18 else \"child\", StringType())\n",
    "\n",
    "df = sqlContext.createDataFrame([{'name': 'Alice', 'age': 1}])\n",
    "df.withColumn(\"maturity\", maturity_udf(df.age))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(\"name.csv\", header=True, index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
