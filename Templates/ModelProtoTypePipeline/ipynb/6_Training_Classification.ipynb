{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a005cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package import *\n",
    "import importlib\n",
    "import utility as u\n",
    "importlib.reload(u)\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"../data/data_train_test.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91e5e9",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3b025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_strategy = \"random\" \n",
    "train_test_split_util = u.TrainTestSplitUtilClass(\n",
    "    test_size=0.2, \n",
    "    split_strategy=split_strategy\n",
    ")\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split_util.run(data=df)\n",
    "\n",
    "# The following step is needed to avoid extra memory consumption in training a xgboost model: https://github.com/dmlc/xgboost/issues/6908\n",
    "X_train_arr = np.ascontiguousarray(X_train)\n",
    "y_train_arr = np.ascontiguousarray(y_train.astype(int))\n",
    "X_test_arr = np.ascontiguousarray(X_test)\n",
    "y_test_arr = np.ascontiguousarray(y_test.astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d336",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd813beb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=u.CV_FOLDS) if split_strategy==\"time_series_split\" else u.CV_FOLDS\n",
    "\n",
    "grid_search = True\n",
    "# param_grid={\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "#         \"max_depth\":[2, 3],\n",
    "#         \"n_estimators\":[50, 100, 200],\n",
    "#         \"reg_alpha\": [0, 0.1, 0.2],\n",
    "#         \"colsample_bytree\":[0.8, 1.0],\n",
    "#         \"subsample\":[0.8, 1.0]\n",
    "# }\n",
    "\n",
    "param_grid={\n",
    "        \"learning_rate\": [0.01],\n",
    "        \"max_depth\":[2, 3],\n",
    "        \"n_estimators\":[50],\n",
    "        \"reg_alpha\": [0],\n",
    "        \"colsample_bytree\":[0.8],\n",
    "        \"subsample\":[0.8]\n",
    "}\n",
    "\n",
    "if not grid_search:\n",
    "    model = XGBClassifier(\n",
    "        learning_rate=0.05, \n",
    "        n_estimators=200, \n",
    "        max_depth=5,\n",
    "        random_state=0,\n",
    "        nthread=4,\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "else:\n",
    "    model = GridSearchCV(\n",
    "        estimator=XGBClassifier(\n",
    "            objective=\"binary:logistic\", \n",
    "            tree_method=\"hist\",  # to speed up training\n",
    "            nthread=4, \n",
    "            seed=42,\n",
    "            use_label_encoder=False\n",
    "        ), \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        scoring=\"average_precision\", #\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "model = model.fit(X_train_arr, y_train_arr)\n",
    "\n",
    "if grid_search:\n",
    "    print(\"best params from grid search:\", model.best_params_)\n",
    "    model = model.best_estimator_\n",
    "\n",
    "model.save_model(u.MODEL_PATH)\n",
    "\n",
    "print(f\"Execution time: {round((time()-start_time)/60, 2)} mins.\")\n",
    "\n",
    "best_params = model.get_params()\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206483f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ba31f0",
   "metadata": {},
   "source": [
    "### n_estimator learning curves\n",
    "\n",
    "To diagnose overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12165bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "util = u.BinaryClassifierPerformanceAndDiagnosisUtilClass()\n",
    "eval_data_sets = [(X_train_arr, y_train_arr), (X_test_arr, y_test_arr)]\n",
    "util.learning_curve_n_estimators(\n",
    "    estimator=XGBClassifier(**best_params), \n",
    "    eval_data_sets=eval_data_sets, \n",
    "    eval_metrics=[\"logloss\", \"auc\", \"aucpr\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827d0b6",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64e61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c10575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = xgb.Booster()\n",
    "model.load_model(u.MODEL_PATH)\n",
    "predictor = u.PredictorUtilClass()\n",
    "\n",
    "df_train = predictor.get_predictions(X=X_train, y=y_train, model=model)\n",
    "df_test = predictor.get_predictions(X=X_test, y=y_test, model=model)\n",
    "\n",
    "for name, data in zip([\"train\", \"test\"], [df_train, df_test]):\n",
    "    print(name)\n",
    "    df_tmp = data[[\"y\", u.BINARY_PREDICTION_NAME]]\n",
    "    df_tmp.sort_values(by=[u.BINARY_PREDICTION_NAME], ascending=False, inplace=True)\n",
    "\n",
    "    num_of_positive = sum(df_tmp[\"y\"])\n",
    "    percentage_of_positive_examples = round(np.mean(df_tmp[\"y\"])*100)\n",
    "    print(f\"percentage of positive examples: {percentage_of_positive_examples }%\")\n",
    "\n",
    "    percentage_of_positive_examples_at_the_top = round(sum(df_tmp.head(num_of_positive)[\"y\"])/num_of_positive*100)\n",
    "    print(f\"{percentage_of_positive_examples_at_the_top}% of positive examples are captured by {num_of_positive} ({percentage_of_positive_examples}%) of invoices with highest predicted probabilities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da6e86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tmp = df_test[[\"y\", u.BINARY_PREDICTION_NAME]]\n",
    "df_tmp.sort_values(by=[u.BINARY_PREDICTION_NAME], ascending=False, inplace=True)\n",
    "#df_tmp.head(100).reset_index(drop=True)#.mean()\n",
    "#df_tmp.tail(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "util = u.BinaryClassifierPerformanceAndDiagnosisUtilClass(model)\n",
    "util.roc_pr_curves_train_test(df_train, \n",
    "                              df_test, \n",
    "                              target_col_name=\"y\", \n",
    "                              prediction_col_name=u.BINARY_PREDICTION_NAME, \n",
    "                              suptitle=f'my_title',\n",
    "                              save_to_path=f\"../plot/ModelPerformance/ROC_PR_Curves.png\"\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.histogram_of_predicted_prob_train_test(df_train, \n",
    "                                            df_test, \n",
    "                                            target_col_name=\"y\", \n",
    "                                            prediction_col_name=u.BINARY_PREDICTION_NAME,\n",
    "                                            suptitle=f\"Histograms of Predicted Probabilities)\",\n",
    "                                            save_to_path=f\"../plot/ModelPerformance/Histograms_Of_Predicted_Probabilities.png\"\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb20c03",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4eab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(u.FEATURE_COLS)[sorted_idx])\n",
    "\n",
    "plt.title(f\"Feature Importance)\")\n",
    "plt.savefig(fname=f\"../plot/ModelPerformance/Feature_Importance.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a8856",
   "metadata": {},
   "source": [
    "### SHAP Value\n",
    "\n",
    "[shap api doc](https://shap.readthedocs.io/en/latest/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0826159",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)    \n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.title(f\"Shap Summary)\")\n",
    "plt.savefig(fname=f\"../plot/ModelPerformance/Shap_Summary.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4824f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values, max_display=20, show=False)\n",
    "plt.title(f\"Shap Bar)\")\n",
    "plt.savefig(fname=f\"../plot/ModelPerformance/Shap_Bar.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eedaf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7176f",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2128ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_single = XGBClassifier(objective=\"binary:logistic\", \n",
    "                             learning_rate=0.05, \n",
    "                             n_estimators=100, \n",
    "                             max_depth=6,\n",
    "                             random_state=0,\n",
    "                             nthread=4, \n",
    "                             seed=42, \n",
    "                             use_label_encoder=False)\n",
    "plot = util.plot_learning_curve(\n",
    "    model_single, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    cv=5, \n",
    "    scoring=\"average_precision\"\n",
    ")\n",
    "plot.title(f\"Learning Curve\")\n",
    "plt.ylim(0,1)\n",
    "plot.savefig(fname=f\"../plot/ModelPerformance/Learning_curve.png\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62ad2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "input_parameters": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
